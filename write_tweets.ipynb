{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac00fca-8efb-4522-abc8-5580c3117de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434575a6-165b-4361-ae7d-5429b63a3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY IMPORTS\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row, Column, SparkSession\n",
    "from textblob import TextBlob\n",
    "import pyspark.sql.types as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea049670-9781-4684-95ea-267d07851e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/18 02:27:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName = \"Text Cleaning\")\n",
    "strc = StreamingContext(sc, 3)\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"read tweet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c550713b-dd1f-4b7c-b77e-56e4a1e3408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA FROM TCP SOCKET\n",
    "text_data = strc.socketTextStream('127.0.0.1', 5556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6857c97-ce3d-46d8-a62c-1371a791881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!pip install elasticsearch\n",
    "#!pip install kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ac0bd8-db5a-4990-b1cc-812d7e4da36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESS DATA\n",
    "def get_prediction(sentence): \n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"\\s+\",\" \", sentence)\n",
    "    sentence = re.sub(\"\\W\",\" \", sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = ' '.join(word.lower() for word in sentence.split() if word not in stop_words and len(word) > 3)\n",
    "    sentence = re.sub(r'\\b\\w{1,3}\\b', '', sentence)\n",
    "    res = TextBlob(sentence)\n",
    "    senti = res.sentiment.polarity\n",
    "    if senti > 0:\n",
    "        senti = 'positive'\n",
    "    elif senti < 0:\n",
    "        senti = 'negative'\n",
    "    elif senti == 0:\n",
    "        senti = 'neutral'\n",
    "    return str(senti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2469c1cc-01f0-4b0a-920e-ea85fd178dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SENTIMENTS TO KAFKA TOPIC\n",
    "def convt(rdd):\n",
    "    \n",
    "    my_sch = tp.StructType([tp.StructField(name= 'value',dataType= tp.StringType(),  nullable= True)])\n",
    "    df = spark.createDataFrame([rdd], schema=my_sch)\n",
    "    df.writeStream.format(\"console\").start()\n",
    "    \n",
    "#     if rdd.isEmpty():\n",
    "#         my_sch = tp.StructType([tp.StructField('value', tp.StringType(), True)])\n",
    "#         rdd = sc.emptyRDD()\n",
    "        \n",
    "#         df = spark.createDataFrame(rdd, my_sch)  \n",
    "#     else:\n",
    "#         schema = tp.StructType([tp.StructField('value', tp.StringType(), True)])\n",
    "#         rdd = sc.parallelize(rdd.collect())\n",
    "#         rdd = rdd.map(lambda x:[x]) # transform the rdd\n",
    " \n",
    "#         df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "    \n",
    "#     df.printSchema()\n",
    "    \n",
    "#     df.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"topic\", \"sentiments\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc375b5-e6fc-4e61-ae1d-f062dd95bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = text_data.map(lambda x: get_prediction(x))\n",
    "r.foreachRDD(convt)\n",
    "\n",
    "r.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86780fb-bfca-4ccf-9390-27547d98fe8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:43 WARN BlockManager: Block input-0-1650266982800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:43 WARN BlockManager: Block input-0-1650266983000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:43 WARN BlockManager: Block input-0-1650266983600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:29:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:44 ERROR JobScheduler: Error running job streaming job 1650266982000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n    df.writeStream.format(\"console\").start()\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n    return DataStreamWriter(self)\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n    self._jwrite = df._jdf.writeStream()\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/1190718396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n    r = self.func(t, *rdds)\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n    df.writeStream.format(\"console\").start()\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n    return DataStreamWriter(self)\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n    self._jwrite = df._jdf.writeStream()\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:44 WARN BlockManager: Block input-0-1650266984600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:45 ERROR JobScheduler: Error running job streaming job 1650266985000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:29:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:45 WARN BlockManager: Block input-0-1650266985600 replicated to only 0 peer(s) instead of 1 peers\n",
      "objc[51437]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51437]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:29:46 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:46 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:46 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:46 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:29:46 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n",
      "22/04/18 02:29:46 ERROR JobScheduler: Error running job streaming job 1650266985000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:46 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:46 WARN BlockManager: Block input-0-1650266986600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:47 WARN BlockManager: Block input-0-1650266987600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:48 ERROR JobScheduler: Error running job streaming job 1650266988000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51442]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51442]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:29:48 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:48 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:48 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:48 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:29:48 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n",
      "22/04/18 02:29:48 ERROR JobScheduler: Error running job streaming job 1650266988000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:48 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:48 WARN BlockManager: Block input-0-1650266988600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:49 WARN BlockManager: Block input-0-1650266989600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:50 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:50 WARN BlockManager: Block input-0-1650266990600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:51 ERROR JobScheduler: Error running job streaming job 1650266991000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51444]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51444]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:29:51 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:51 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:51 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:51 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:29:51 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "22/04/18 02:29:51 ERROR JobScheduler: Error running job streaming job 1650266991000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:51 WARN BlockManager: Block input-0-1650266991600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:52 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:52 WARN BlockManager: Block input-0-1650266991800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:53 WARN BlockManager: Block input-0-1650266992800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:53 WARN BlockManager: Block input-0-1650266993600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:54 WARN BlockManager: Block input-0-1650266993800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:54 ERROR JobScheduler: Error running job streaming job 1650266994000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51446]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51446]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:29:54 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:54 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:54 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:54 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:29:54 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\n",
      "22/04/18 02:29:54 ERROR JobScheduler: Error running job streaming job 1650266994000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:54 WARN BlockManager: Block input-0-1650266994600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:55 WARN BlockManager: Block input-0-1650266995600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:57 WARN BlockManager: Block input-0-1650266996800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:57 ERROR JobScheduler: Error running job streaming job 1650266997000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51449]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51449]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:29:57 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:57 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:57 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:29:57 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:29:57 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n",
      "22/04/18 02:29:57 ERROR JobScheduler: Error running job streaming job 1650266997000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:29:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:57 WARN BlockManager: Block input-0-1650266997600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:58 WARN BlockManager: Block input-0-1650266998600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:59 WARN BlockManager: Block input-0-1650266998800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:29:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:29:59 WARN BlockManager: Block input-0-1650266999600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:00 ERROR JobScheduler: Error running job streaming job 1650267000000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51451]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51451]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:00 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:00 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:00 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:00 ERROR JobScheduler: Error running job streaming job 1650267000000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:00 WARN BlockManager: Block input-0-1650267000600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:01 WARN BlockManager: Block input-0-1650267001600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:02 WARN BlockManager: Block input-0-1650267002600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:03 ERROR JobScheduler: Error running job streaming job 1650267003000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51460]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51460]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:03 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:03 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:03 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:03 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:03 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:03 ERROR JobScheduler: Error running job streaming job 1650267003000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:03 WARN BlockManager: Block input-0-1650267003600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:04 WARN BlockManager: Block input-0-1650267004600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:05 WARN BlockManager: Block input-0-1650267004800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:05 WARN BlockManager: Block input-0-1650267005600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:06 ERROR JobScheduler: Error running job streaming job 1650267006000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51464]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51464]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:06 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:06 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:06 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:06 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:06 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:06 ERROR JobScheduler: Error running job streaming job 1650267006000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:06 WARN BlockManager: Block input-0-1650267006600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:07 WARN BlockManager: Block input-0-1650267007600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:08 WARN BlockManager: Block input-0-1650267007800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:08 WARN BlockManager: Block input-0-1650267008600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:09 WARN BlockManager: Block input-0-1650267008800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:09 ERROR JobScheduler: Error running job streaming job 1650267009000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51466]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51466]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:09 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:09 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:09 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:09 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 9) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:09 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:09 ERROR JobScheduler: Error running job streaming job 1650267009000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:09 WARN BlockManager: Block input-0-1650267009600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:10 WARN BlockManager: Block input-0-1650267010600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:11 WARN BlockManager: Block input-0-1650267011600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:12 ERROR JobScheduler: Error running job streaming job 1650267012000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51468]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51468]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:12 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:12 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:12 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 10)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:12 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 10) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:12 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:12 ERROR JobScheduler: Error running job streaming job 1650267012000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:12 WARN BlockManager: Block input-0-1650267012600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:13 WARN BlockManager: Block input-0-1650267013600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:14 WARN BlockManager: Block input-0-1650267014600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:15 ERROR JobScheduler: Error running job streaming job 1650267015000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51470]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51470]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:15 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:15 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:15 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:15 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:15 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:15 ERROR JobScheduler: Error running job streaming job 1650267015000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:15 WARN BlockManager: Block input-0-1650267015600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/04/18 02:30:16 WARN BlockManager: Block input-0-1650267015800 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/04/18 02:30:18 ERROR JobScheduler: Error running job streaming job 1650267018000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "objc[51478]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.\n",
      "objc[51478]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n",
      "22/04/18 02:30:18 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:18 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:18 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 12)\n",
      "org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "22/04/18 02:30:18 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 12) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "22/04/18 02:30:18 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job\n",
      "22/04/18 02:30:18 ERROR JobScheduler: Error running job streaming job 1650267018000 ms.1\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 170, in takeAndPrint\n",
      "    taken = rdd.take(num + 1)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1568, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/context.py\", line 1227, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (harishs-air.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy18.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:595)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:577)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:718)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:680)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:434)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:269)\n",
      "Caused by: java.io.EOFException\n",
      "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\t... 11 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:21 ERROR JobScheduler: Error running job streaming job 1650267021000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:24 ERROR JobScheduler: Error running job streaming job 1650267024000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:27 ERROR JobScheduler: Error running job streaming job 1650267027000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:30 ERROR JobScheduler: Error running job streaming job 1650267030000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:33 ERROR JobScheduler: Error running job streaming job 1650267033000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:36 ERROR JobScheduler: Error running job streaming job 1650267036000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:39 ERROR JobScheduler: Error running job streaming job 1650267039000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:42 ERROR JobScheduler: Error running job streaming job 1650267042000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:45 ERROR JobScheduler: Error running job streaming job 1650267045000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:48 ERROR JobScheduler: Error running job streaming job 1650267048000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:51 ERROR JobScheduler: Error running job streaming job 1650267051000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:54 ERROR JobScheduler: Error running job streaming job 1650267054000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:30:57 ERROR JobScheduler: Error running job streaming job 1650267057000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:30:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:00 ERROR JobScheduler: Error running job streaming job 1650267060000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:03 ERROR JobScheduler: Error running job streaming job 1650267063000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:06 ERROR JobScheduler: Error running job streaming job 1650267066000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:09 ERROR JobScheduler: Error running job streaming job 1650267069000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:12 ERROR JobScheduler: Error running job streaming job 1650267072000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:15 ERROR JobScheduler: Error running job streaming job 1650267075000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:18 ERROR JobScheduler: Error running job streaming job 1650267078000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:21 ERROR JobScheduler: Error running job streaming job 1650267081000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:24 ERROR JobScheduler: Error running job streaming job 1650267084000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:27 ERROR JobScheduler: Error running job streaming job 1650267087000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:30 ERROR JobScheduler: Error running job streaming job 1650267090000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:33 ERROR JobScheduler: Error running job streaming job 1650267093000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:36 ERROR JobScheduler: Error running job streaming job 1650267096000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:39 ERROR JobScheduler: Error running job streaming job 1650267099000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:42 ERROR JobScheduler: Error running job streaming job 1650267102000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:45 ERROR JobScheduler: Error running job streaming job 1650267105000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:48 ERROR JobScheduler: Error running job streaming job 1650267108000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:51 ERROR JobScheduler: Error running job streaming job 1650267111000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:54 ERROR JobScheduler: Error running job streaming job 1650267114000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:31:57 ERROR JobScheduler: Error running job streaming job 1650267117000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:31:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:00 ERROR JobScheduler: Error running job streaming job 1650267120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:03 ERROR JobScheduler: Error running job streaming job 1650267123000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:06 ERROR JobScheduler: Error running job streaming job 1650267126000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:09 ERROR JobScheduler: Error running job streaming job 1650267129000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:12 ERROR JobScheduler: Error running job streaming job 1650267132000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:15 ERROR JobScheduler: Error running job streaming job 1650267135000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:18 ERROR JobScheduler: Error running job streaming job 1650267138000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:21 ERROR JobScheduler: Error running job streaming job 1650267141000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:24 ERROR JobScheduler: Error running job streaming job 1650267144000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:27 ERROR JobScheduler: Error running job streaming job 1650267147000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:30 ERROR JobScheduler: Error running job streaming job 1650267150000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:32:33 ERROR JobScheduler: Error running job streaming job 1650267153000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267156000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267159000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267162000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267165000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267168000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267171000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267174000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267177000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267180000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:32:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267183000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:53 ERROR JobScheduler: Error running job streaming job 1650267186000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267189000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267192000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267195000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267198000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267201000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267204000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267207000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267210000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267213000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267216000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267219000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267222000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:54 ERROR JobScheduler: Error running job streaming job 1650267225000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267228000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267231000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267234000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267237000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267240000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267243000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267246000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267249000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:33:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267252000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267255000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267258000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267261000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267264000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267267000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267270000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267273000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267276000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267279000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267282000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267285000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267288000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267291000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267294000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:55 ERROR JobScheduler: Error running job streaming job 1650267297000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267300000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:34:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267303000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267306000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267309000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267312000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267315000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267318000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267321000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267324000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267327000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267330000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267333000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267336000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267339000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267342000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267345000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:56 ERROR JobScheduler: Error running job streaming job 1650267348000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267351000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267354000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267357000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267360000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267363000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267366000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:35:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267369000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267372000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267375000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267378000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267381000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267384000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267387000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267390000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:57 ERROR JobScheduler: Error running job streaming job 1650267393000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267396000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267399000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267402000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267405000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267408000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267411000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267414000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267417000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267423000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267426000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267429000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:36:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267432000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267435000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267438000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267441000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267444000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267447000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267453000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267456000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267459000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267462000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267465000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267468000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267471000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267474000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267477000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267483000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 02:47:58 ERROR JobScheduler: Error running job streaming job 1650267486000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:37:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "22/04/18 03:03:06 ERROR JobScheduler: Error running job streaming job 1650267489000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:06 ERROR JobScheduler: Error running job streaming job 1650267492000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:06 ERROR JobScheduler: Error running job streaming job 1650267495000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267498000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267501000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267504000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267507000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267510000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267513000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267516000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267519000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267522000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267525000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267528000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267531000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:07 ERROR JobScheduler: Error running job streaming job 1650267534000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267537000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267540000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:38:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267543000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267546000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267549000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267552000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267555000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267558000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267561000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267564000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267567000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267570000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267573000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267576000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267579000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267582000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267585000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267588000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267591000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267594000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267597000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267603000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:39:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267606000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267609000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267612000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267615000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:08 ERROR JobScheduler: Error running job streaming job 1650267618000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267621000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267624000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267627000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267630000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267633000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267636000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267639000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267642000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267645000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267648000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267651000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267654000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:40:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267657000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267660000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267663000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267666000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267669000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267672000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267675000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267678000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267681000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267684000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:09 ERROR JobScheduler: Error running job streaming job 1650267687000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267690000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267693000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267696000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267699000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267702000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267705000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267708000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267711000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267714000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267717000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267720000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267723000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267726000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267729000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:41:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267732000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267735000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267738000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267741000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267744000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267747000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267750000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267753000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267756000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267759000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267762000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267765000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:10 ERROR JobScheduler: Error running job streaming job 1650267768000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267771000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267774000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267777000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267780000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267783000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:42:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267786000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267789000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267792000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267795000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267798000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267801000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267804000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267807000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267810000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267813000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267816000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267819000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267822000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267825000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267828000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:43:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267831000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267834000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267837000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267840000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267843000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267846000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267849000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267852000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267855000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267858000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267861000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267864000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:11 ERROR JobScheduler: Error running job streaming job 1650267867000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267870000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267873000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267876000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267879000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267882000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267885000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267888000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267891000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267894000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267897000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:44:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267900000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267903000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267906000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267909000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267912000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267915000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267918000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267921000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267924000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267927000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267930000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:12 ERROR JobScheduler: Error running job streaming job 1650267933000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267936000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267939000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267942000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267945000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267948000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267951000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267954000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267957000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267960000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267963000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267966000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267969000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267972000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:45:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267975000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267978000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267981000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267984000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267987000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267993000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267996000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650267999000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268002000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268005000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268008000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268011000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268014000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:46:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268017000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268020000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268023000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268026000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:13 ERROR JobScheduler: Error running job streaming job 1650268029000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268032000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268035000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268038000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268041000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268044000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268047000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268050000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268053000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268056000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268059000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268062000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268065000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268068000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268071000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268074000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268077000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268080000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268083000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:47:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268086000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268089000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268092000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268095000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268098000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268101000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268104000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268107000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268113000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268116000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268119000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268122000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268125000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268128000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268131000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:14 ERROR JobScheduler: Error running job streaming job 1650268134000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268137000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268143000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:48:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268146000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268149000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268152000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268155000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268158000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268161000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268164000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268167000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268170000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268173000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268176000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268179000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268182000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268185000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268188000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268191000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268194000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268197000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:49:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268200000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268203000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268206000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268209000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268212000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268215000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268218000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268221000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268224000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268227000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268230000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268233000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268236000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268239000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268242000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:15 ERROR JobScheduler: Error running job streaming job 1650268245000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268248000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268251000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268254000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268257000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:50:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268260000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268263000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268266000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268269000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268272000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268275000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268278000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268281000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268284000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268287000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268290000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268293000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268296000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268299000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:51:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268302000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268305000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268308000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268311000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268314000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268317000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268320000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268323000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268326000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268329000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268332000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268335000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268338000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:16 ERROR JobScheduler: Error running job streaming job 1650268341000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268344000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268347000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268350000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268353000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268356000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268359000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268362000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268365000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268368000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268371000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268374000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:52:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268377000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268380000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268383000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268386000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268389000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268392000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268395000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268398000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268401000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268404000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268407000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268410000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268413000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268416000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268419000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268422000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268425000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268428000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:17 ERROR JobScheduler: Error running job streaming job 1650268431000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268434000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268437000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:53:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268443000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268446000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268449000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268452000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268455000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268458000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268461000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268464000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268467000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268473000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268476000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268479000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268482000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268485000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268488000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268491000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268494000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268497000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268500000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268503000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268506000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:54:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268509000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268512000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268515000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268518000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268521000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268524000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:18 ERROR JobScheduler: Error running job streaming job 1650268527000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268530000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268533000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268536000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268539000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268542000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268545000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268548000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268551000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268554000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268557000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268560000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268563000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:55:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268566000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268569000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268572000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268575000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268578000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268581000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268584000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268587000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268590000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268593000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268596000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268599000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:19 ERROR JobScheduler: Error running job streaming job 1650268602000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268605000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268608000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268611000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268614000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268617000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268620000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268623000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268626000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:56:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268629000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268632000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268635000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268638000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268641000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268644000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268647000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268650000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268653000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268656000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268659000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268662000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268665000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268668000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268671000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268674000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268677000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268680000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268683000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268686000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268689000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268692000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:57:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268695000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268698000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:20 ERROR JobScheduler: Error running job streaming job 1650268701000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268704000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268707000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268710000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268713000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268716000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268719000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268722000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268725000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268728000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268731000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268734000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268737000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268740000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:58:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268743000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268746000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268749000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268752000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268755000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268758000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268761000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268764000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268767000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268770000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268773000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268776000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268779000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268782000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268785000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268788000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268791000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268794000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268797000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268800000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268803000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:21 ERROR JobScheduler: Error running job streaming job 1650268806000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 02:59:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268809000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268812000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268815000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268818000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268821000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268824000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268827000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268830000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268833000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268836000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268839000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268842000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268845000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268848000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268851000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268854000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268857000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268860000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:00:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268863000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268866000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268869000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268872000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268875000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268878000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268881000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268884000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268887000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268890000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268893000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268896000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268899000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268902000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268905000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268908000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268911000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268914000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268917000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268920000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:22 ERROR JobScheduler: Error running job streaming job 1650268923000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268926000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268929000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268932000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:01:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268935000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268938000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268941000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268944000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268947000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268950000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268953000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268956000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268959000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268962000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:02:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268965000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268968000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268971000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268974000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268977000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268983000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268986000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268989000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268992000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268995000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650268998000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/04/18 03:03:23 ERROR JobScheduler: Error running job streaming job 1650269001000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:24 ERROR JobScheduler: Error running job streaming job 1650269004000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:27 ERROR JobScheduler: Error running job streaming job 1650269007000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:30 ERROR JobScheduler: Error running job streaming job 1650269010000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:33 ERROR JobScheduler: Error running job streaming job 1650269013000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:36 ERROR JobScheduler: Error running job streaming job 1650269016000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:39 ERROR JobScheduler: Error running job streaming job 1650269019000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:42 ERROR JobScheduler: Error running job streaming job 1650269022000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:45 ERROR JobScheduler: Error running job streaming job 1650269025000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:48 ERROR JobScheduler: Error running job streaming job 1650269028000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:51 ERROR JobScheduler: Error running job streaming job 1650269031000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:54 ERROR JobScheduler: Error running job streaming job 1650269034000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:03:57 ERROR JobScheduler: Error running job streaming job 1650269037000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:03:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:00 ERROR JobScheduler: Error running job streaming job 1650269040000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:03 ERROR JobScheduler: Error running job streaming job 1650269043000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:06 ERROR JobScheduler: Error running job streaming job 1650269046000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:09 ERROR JobScheduler: Error running job streaming job 1650269049000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:12 ERROR JobScheduler: Error running job streaming job 1650269052000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:15 ERROR JobScheduler: Error running job streaming job 1650269055000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:18 ERROR JobScheduler: Error running job streaming job 1650269058000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:21 ERROR JobScheduler: Error running job streaming job 1650269061000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:24 ERROR JobScheduler: Error running job streaming job 1650269064000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:27 ERROR JobScheduler: Error running job streaming job 1650269067000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:30 ERROR JobScheduler: Error running job streaming job 1650269070000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:33 ERROR JobScheduler: Error running job streaming job 1650269073000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:36 ERROR JobScheduler: Error running job streaming job 1650269076000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:39 ERROR JobScheduler: Error running job streaming job 1650269079000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:42 ERROR JobScheduler: Error running job streaming job 1650269082000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:45 ERROR JobScheduler: Error running job streaming job 1650269085000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:48 ERROR JobScheduler: Error running job streaming job 1650269088000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:51 ERROR JobScheduler: Error running job streaming job 1650269091000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:54 ERROR JobScheduler: Error running job streaming job 1650269094000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:04:57 ERROR JobScheduler: Error running job streaming job 1650269097000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:04:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:00 ERROR JobScheduler: Error running job streaming job 1650269100000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:03 ERROR JobScheduler: Error running job streaming job 1650269103000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:06 ERROR JobScheduler: Error running job streaming job 1650269106000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:09 ERROR JobScheduler: Error running job streaming job 1650269109000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:12 ERROR JobScheduler: Error running job streaming job 1650269112000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:15 ERROR JobScheduler: Error running job streaming job 1650269115000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:18 ERROR JobScheduler: Error running job streaming job 1650269118000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:21 ERROR JobScheduler: Error running job streaming job 1650269121000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:24 ERROR JobScheduler: Error running job streaming job 1650269124000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:27 ERROR JobScheduler: Error running job streaming job 1650269127000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:30 ERROR JobScheduler: Error running job streaming job 1650269130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:33 ERROR JobScheduler: Error running job streaming job 1650269133000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:36 ERROR JobScheduler: Error running job streaming job 1650269136000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:39 ERROR JobScheduler: Error running job streaming job 1650269139000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:42 ERROR JobScheduler: Error running job streaming job 1650269142000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:45 ERROR JobScheduler: Error running job streaming job 1650269145000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:48 ERROR JobScheduler: Error running job streaming job 1650269148000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:51 ERROR JobScheduler: Error running job streaming job 1650269151000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:54 ERROR JobScheduler: Error running job streaming job 1650269154000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:05:57 ERROR JobScheduler: Error running job streaming job 1650269157000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:05:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:00 ERROR JobScheduler: Error running job streaming job 1650269160000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:03 ERROR JobScheduler: Error running job streaming job 1650269163000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:06 ERROR JobScheduler: Error running job streaming job 1650269166000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:09 ERROR JobScheduler: Error running job streaming job 1650269169000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:12 ERROR JobScheduler: Error running job streaming job 1650269172000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:15 ERROR JobScheduler: Error running job streaming job 1650269175000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:18 ERROR JobScheduler: Error running job streaming job 1650269178000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:21 ERROR JobScheduler: Error running job streaming job 1650269181000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:24 ERROR JobScheduler: Error running job streaming job 1650269184000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:27 ERROR JobScheduler: Error running job streaming job 1650269187000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:30 ERROR JobScheduler: Error running job streaming job 1650269190000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:33 ERROR JobScheduler: Error running job streaming job 1650269193000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:36 ERROR JobScheduler: Error running job streaming job 1650269196000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:39 ERROR JobScheduler: Error running job streaming job 1650269199000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:42 ERROR JobScheduler: Error running job streaming job 1650269202000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:45 ERROR JobScheduler: Error running job streaming job 1650269205000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:48 ERROR JobScheduler: Error running job streaming job 1650269208000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:51 ERROR JobScheduler: Error running job streaming job 1650269211000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:54 ERROR JobScheduler: Error running job streaming job 1650269214000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:06:57 ERROR JobScheduler: Error running job streaming job 1650269217000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:06:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:00 ERROR JobScheduler: Error running job streaming job 1650269220000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:03 ERROR JobScheduler: Error running job streaming job 1650269223000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:06 ERROR JobScheduler: Error running job streaming job 1650269226000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:09 ERROR JobScheduler: Error running job streaming job 1650269229000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:12 ERROR JobScheduler: Error running job streaming job 1650269232000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:15 ERROR JobScheduler: Error running job streaming job 1650269235000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:18 ERROR JobScheduler: Error running job streaming job 1650269238000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:21 ERROR JobScheduler: Error running job streaming job 1650269241000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:24 ERROR JobScheduler: Error running job streaming job 1650269244000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:27 ERROR JobScheduler: Error running job streaming job 1650269247000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:30 ERROR JobScheduler: Error running job streaming job 1650269250000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:33 ERROR JobScheduler: Error running job streaming job 1650269253000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:36 ERROR JobScheduler: Error running job streaming job 1650269256000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:39 ERROR JobScheduler: Error running job streaming job 1650269259000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:42 ERROR JobScheduler: Error running job streaming job 1650269262000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:45 ERROR JobScheduler: Error running job streaming job 1650269265000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:48 ERROR JobScheduler: Error running job streaming job 1650269268000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:51 ERROR JobScheduler: Error running job streaming job 1650269271000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:54 ERROR JobScheduler: Error running job streaming job 1650269274000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:07:57 ERROR JobScheduler: Error running job streaming job 1650269277000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:07:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:00 ERROR JobScheduler: Error running job streaming job 1650269280000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:03 ERROR JobScheduler: Error running job streaming job 1650269283000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:06 ERROR JobScheduler: Error running job streaming job 1650269286000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:09 ERROR JobScheduler: Error running job streaming job 1650269289000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:12 ERROR JobScheduler: Error running job streaming job 1650269292000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:15 ERROR JobScheduler: Error running job streaming job 1650269295000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:18 ERROR JobScheduler: Error running job streaming job 1650269298000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:21 ERROR JobScheduler: Error running job streaming job 1650269301000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:24 ERROR JobScheduler: Error running job streaming job 1650269304000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:27 ERROR JobScheduler: Error running job streaming job 1650269307000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:30 ERROR JobScheduler: Error running job streaming job 1650269310000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:33 ERROR JobScheduler: Error running job streaming job 1650269313000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:36 ERROR JobScheduler: Error running job streaming job 1650269316000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:39 ERROR JobScheduler: Error running job streaming job 1650269319000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:42 ERROR JobScheduler: Error running job streaming job 1650269322000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:45 ERROR JobScheduler: Error running job streaming job 1650269325000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:48 ERROR JobScheduler: Error running job streaming job 1650269328000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:51 ERROR JobScheduler: Error running job streaming job 1650269331000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:54 ERROR JobScheduler: Error running job streaming job 1650269334000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:08:57 ERROR JobScheduler: Error running job streaming job 1650269337000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:08:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:00 ERROR JobScheduler: Error running job streaming job 1650269340000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:03 ERROR JobScheduler: Error running job streaming job 1650269343000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:06 ERROR JobScheduler: Error running job streaming job 1650269346000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:09 ERROR JobScheduler: Error running job streaming job 1650269349000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:12 ERROR JobScheduler: Error running job streaming job 1650269352000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:15 ERROR JobScheduler: Error running job streaming job 1650269355000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:18 ERROR JobScheduler: Error running job streaming job 1650269358000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:21 ERROR JobScheduler: Error running job streaming job 1650269361000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:24 ERROR JobScheduler: Error running job streaming job 1650269364000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:27 ERROR JobScheduler: Error running job streaming job 1650269367000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:30 ERROR JobScheduler: Error running job streaming job 1650269370000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:33 ERROR JobScheduler: Error running job streaming job 1650269373000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:36 ERROR JobScheduler: Error running job streaming job 1650269376000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:39 ERROR JobScheduler: Error running job streaming job 1650269379000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:42 ERROR JobScheduler: Error running job streaming job 1650269382000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:45 ERROR JobScheduler: Error running job streaming job 1650269385000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:48 ERROR JobScheduler: Error running job streaming job 1650269388000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:51 ERROR JobScheduler: Error running job streaming job 1650269391000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:54 ERROR JobScheduler: Error running job streaming job 1650269394000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:09:57 ERROR JobScheduler: Error running job streaming job 1650269397000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:09:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:00 ERROR JobScheduler: Error running job streaming job 1650269400000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:03 ERROR JobScheduler: Error running job streaming job 1650269403000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:06 ERROR JobScheduler: Error running job streaming job 1650269406000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:09 ERROR JobScheduler: Error running job streaming job 1650269409000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:12 ERROR JobScheduler: Error running job streaming job 1650269412000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:15 ERROR JobScheduler: Error running job streaming job 1650269415000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:18 ERROR JobScheduler: Error running job streaming job 1650269418000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:21 ERROR JobScheduler: Error running job streaming job 1650269421000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:24 ERROR JobScheduler: Error running job streaming job 1650269424000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:27 ERROR JobScheduler: Error running job streaming job 1650269427000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:30 ERROR JobScheduler: Error running job streaming job 1650269430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:33 ERROR JobScheduler: Error running job streaming job 1650269433000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:36 ERROR JobScheduler: Error running job streaming job 1650269436000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:39 ERROR JobScheduler: Error running job streaming job 1650269439000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:42 ERROR JobScheduler: Error running job streaming job 1650269442000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:45 ERROR JobScheduler: Error running job streaming job 1650269445000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:48 ERROR JobScheduler: Error running job streaming job 1650269448000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:51 ERROR JobScheduler: Error running job streaming job 1650269451000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:54 ERROR JobScheduler: Error running job streaming job 1650269454000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:10:57 ERROR JobScheduler: Error running job streaming job 1650269457000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:10:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:00 ERROR JobScheduler: Error running job streaming job 1650269460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:03 ERROR JobScheduler: Error running job streaming job 1650269463000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:06 ERROR JobScheduler: Error running job streaming job 1650269466000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:09 ERROR JobScheduler: Error running job streaming job 1650269469000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:12 ERROR JobScheduler: Error running job streaming job 1650269472000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:15 ERROR JobScheduler: Error running job streaming job 1650269475000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:18 ERROR JobScheduler: Error running job streaming job 1650269478000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:21 ERROR JobScheduler: Error running job streaming job 1650269481000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:24 ERROR JobScheduler: Error running job streaming job 1650269484000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:27 ERROR JobScheduler: Error running job streaming job 1650269487000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:30 ERROR JobScheduler: Error running job streaming job 1650269490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:33 ERROR JobScheduler: Error running job streaming job 1650269493000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:36 ERROR JobScheduler: Error running job streaming job 1650269496000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:39 ERROR JobScheduler: Error running job streaming job 1650269499000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:42 ERROR JobScheduler: Error running job streaming job 1650269502000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:45 ERROR JobScheduler: Error running job streaming job 1650269505000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:48 ERROR JobScheduler: Error running job streaming job 1650269508000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:51 ERROR JobScheduler: Error running job streaming job 1650269511000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:54 ERROR JobScheduler: Error running job streaming job 1650269514000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:11:57 ERROR JobScheduler: Error running job streaming job 1650269517000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:11:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:00 ERROR JobScheduler: Error running job streaming job 1650269520000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:03 ERROR JobScheduler: Error running job streaming job 1650269523000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:06 ERROR JobScheduler: Error running job streaming job 1650269526000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:09 ERROR JobScheduler: Error running job streaming job 1650269529000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:12 ERROR JobScheduler: Error running job streaming job 1650269532000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:15 ERROR JobScheduler: Error running job streaming job 1650269535000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:18 ERROR JobScheduler: Error running job streaming job 1650269538000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:21 ERROR JobScheduler: Error running job streaming job 1650269541000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:24 ERROR JobScheduler: Error running job streaming job 1650269544000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:27 ERROR JobScheduler: Error running job streaming job 1650269547000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:30 ERROR JobScheduler: Error running job streaming job 1650269550000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:33 ERROR JobScheduler: Error running job streaming job 1650269553000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:36 ERROR JobScheduler: Error running job streaming job 1650269556000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:39 ERROR JobScheduler: Error running job streaming job 1650269559000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:42 ERROR JobScheduler: Error running job streaming job 1650269562000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:45 ERROR JobScheduler: Error running job streaming job 1650269565000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:48 ERROR JobScheduler: Error running job streaming job 1650269568000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:51 ERROR JobScheduler: Error running job streaming job 1650269571000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:54 ERROR JobScheduler: Error running job streaming job 1650269574000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:12:57 ERROR JobScheduler: Error running job streaming job 1650269577000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:12:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:00 ERROR JobScheduler: Error running job streaming job 1650269580000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:03 ERROR JobScheduler: Error running job streaming job 1650269583000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:06 ERROR JobScheduler: Error running job streaming job 1650269586000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:09 ERROR JobScheduler: Error running job streaming job 1650269589000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:12 ERROR JobScheduler: Error running job streaming job 1650269592000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:15 ERROR JobScheduler: Error running job streaming job 1650269595000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:18 ERROR JobScheduler: Error running job streaming job 1650269598000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:21 ERROR JobScheduler: Error running job streaming job 1650269601000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:24 ERROR JobScheduler: Error running job streaming job 1650269604000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:27 ERROR JobScheduler: Error running job streaming job 1650269607000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:30 ERROR JobScheduler: Error running job streaming job 1650269610000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:33 ERROR JobScheduler: Error running job streaming job 1650269613000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:36 ERROR JobScheduler: Error running job streaming job 1650269616000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:39 ERROR JobScheduler: Error running job streaming job 1650269619000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:42 ERROR JobScheduler: Error running job streaming job 1650269622000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:45 ERROR JobScheduler: Error running job streaming job 1650269625000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:48 ERROR JobScheduler: Error running job streaming job 1650269628000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:51 ERROR JobScheduler: Error running job streaming job 1650269631000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:54 ERROR JobScheduler: Error running job streaming job 1650269634000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:13:57 ERROR JobScheduler: Error running job streaming job 1650269637000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:13:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:00 ERROR JobScheduler: Error running job streaming job 1650269640000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:03 ERROR JobScheduler: Error running job streaming job 1650269643000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:06 ERROR JobScheduler: Error running job streaming job 1650269646000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:09 ERROR JobScheduler: Error running job streaming job 1650269649000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:12 ERROR JobScheduler: Error running job streaming job 1650269652000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:15 ERROR JobScheduler: Error running job streaming job 1650269655000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:18 ERROR JobScheduler: Error running job streaming job 1650269658000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:21 ERROR JobScheduler: Error running job streaming job 1650269661000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:24 ERROR JobScheduler: Error running job streaming job 1650269664000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:27 ERROR JobScheduler: Error running job streaming job 1650269667000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:30 ERROR JobScheduler: Error running job streaming job 1650269670000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:33 ERROR JobScheduler: Error running job streaming job 1650269673000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:36 ERROR JobScheduler: Error running job streaming job 1650269676000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:39 ERROR JobScheduler: Error running job streaming job 1650269679000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:42 ERROR JobScheduler: Error running job streaming job 1650269682000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:45 ERROR JobScheduler: Error running job streaming job 1650269685000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:48 ERROR JobScheduler: Error running job streaming job 1650269688000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:51 ERROR JobScheduler: Error running job streaming job 1650269691000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:54 ERROR JobScheduler: Error running job streaming job 1650269694000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:14:57 ERROR JobScheduler: Error running job streaming job 1650269697000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:14:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:00 ERROR JobScheduler: Error running job streaming job 1650269700000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:03 ERROR JobScheduler: Error running job streaming job 1650269703000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:06 ERROR JobScheduler: Error running job streaming job 1650269706000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:09 ERROR JobScheduler: Error running job streaming job 1650269709000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:12 ERROR JobScheduler: Error running job streaming job 1650269712000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:15 ERROR JobScheduler: Error running job streaming job 1650269715000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:18 ERROR JobScheduler: Error running job streaming job 1650269718000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:21 ERROR JobScheduler: Error running job streaming job 1650269721000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:24 ERROR JobScheduler: Error running job streaming job 1650269724000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:27 ERROR JobScheduler: Error running job streaming job 1650269727000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:30 ERROR JobScheduler: Error running job streaming job 1650269730000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:33 ERROR JobScheduler: Error running job streaming job 1650269733000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:36 ERROR JobScheduler: Error running job streaming job 1650269736000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:39 ERROR JobScheduler: Error running job streaming job 1650269739000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:42 ERROR JobScheduler: Error running job streaming job 1650269742000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:45 ERROR JobScheduler: Error running job streaming job 1650269745000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:48 ERROR JobScheduler: Error running job streaming job 1650269748000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:51 ERROR JobScheduler: Error running job streaming job 1650269751000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:54 ERROR JobScheduler: Error running job streaming job 1650269754000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:15:57 ERROR JobScheduler: Error running job streaming job 1650269757000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:15:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:00 ERROR JobScheduler: Error running job streaming job 1650269760000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:03 ERROR JobScheduler: Error running job streaming job 1650269763000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:06 ERROR JobScheduler: Error running job streaming job 1650269766000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:09 ERROR JobScheduler: Error running job streaming job 1650269769000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:12 ERROR JobScheduler: Error running job streaming job 1650269772000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:15 ERROR JobScheduler: Error running job streaming job 1650269775000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:18 ERROR JobScheduler: Error running job streaming job 1650269778000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:21 ERROR JobScheduler: Error running job streaming job 1650269781000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:24 ERROR JobScheduler: Error running job streaming job 1650269784000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:27 ERROR JobScheduler: Error running job streaming job 1650269787000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:30 ERROR JobScheduler: Error running job streaming job 1650269790000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:33 ERROR JobScheduler: Error running job streaming job 1650269793000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:36 ERROR JobScheduler: Error running job streaming job 1650269796000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:39 ERROR JobScheduler: Error running job streaming job 1650269799000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:42 ERROR JobScheduler: Error running job streaming job 1650269802000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:45 ERROR JobScheduler: Error running job streaming job 1650269805000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:48 ERROR JobScheduler: Error running job streaming job 1650269808000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:51 ERROR JobScheduler: Error running job streaming job 1650269811000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:54 ERROR JobScheduler: Error running job streaming job 1650269814000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:16:57 ERROR JobScheduler: Error running job streaming job 1650269817000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:16:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:00 ERROR JobScheduler: Error running job streaming job 1650269820000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:03 ERROR JobScheduler: Error running job streaming job 1650269823000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:06 ERROR JobScheduler: Error running job streaming job 1650269826000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:09 ERROR JobScheduler: Error running job streaming job 1650269829000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:12 ERROR JobScheduler: Error running job streaming job 1650269832000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:15 ERROR JobScheduler: Error running job streaming job 1650269835000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:18 ERROR JobScheduler: Error running job streaming job 1650269838000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:21 ERROR JobScheduler: Error running job streaming job 1650269841000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:24 ERROR JobScheduler: Error running job streaming job 1650269844000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:27 ERROR JobScheduler: Error running job streaming job 1650269847000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:30 ERROR JobScheduler: Error running job streaming job 1650269850000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:33 ERROR JobScheduler: Error running job streaming job 1650269853000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:36 ERROR JobScheduler: Error running job streaming job 1650269856000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:39 ERROR JobScheduler: Error running job streaming job 1650269859000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:42 ERROR JobScheduler: Error running job streaming job 1650269862000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:45 ERROR JobScheduler: Error running job streaming job 1650269865000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:48 ERROR JobScheduler: Error running job streaming job 1650269868000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:51 ERROR JobScheduler: Error running job streaming job 1650269871000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:54 ERROR JobScheduler: Error running job streaming job 1650269874000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:17:57 ERROR JobScheduler: Error running job streaming job 1650269877000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:17:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:00 ERROR JobScheduler: Error running job streaming job 1650269880000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:03 ERROR JobScheduler: Error running job streaming job 1650269883000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:06 ERROR JobScheduler: Error running job streaming job 1650269886000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:09 ERROR JobScheduler: Error running job streaming job 1650269889000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:12 ERROR JobScheduler: Error running job streaming job 1650269892000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:15 ERROR JobScheduler: Error running job streaming job 1650269895000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:18 ERROR JobScheduler: Error running job streaming job 1650269898000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:21 ERROR JobScheduler: Error running job streaming job 1650269901000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:24 ERROR JobScheduler: Error running job streaming job 1650269904000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:27 ERROR JobScheduler: Error running job streaming job 1650269907000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:30 ERROR JobScheduler: Error running job streaming job 1650269910000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:33 ERROR JobScheduler: Error running job streaming job 1650269913000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:36 ERROR JobScheduler: Error running job streaming job 1650269916000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:39 ERROR JobScheduler: Error running job streaming job 1650269919000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:42 ERROR JobScheduler: Error running job streaming job 1650269922000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:45 ERROR JobScheduler: Error running job streaming job 1650269925000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:48 ERROR JobScheduler: Error running job streaming job 1650269928000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:51 ERROR JobScheduler: Error running job streaming job 1650269931000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:54 ERROR JobScheduler: Error running job streaming job 1650269934000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:18:57 ERROR JobScheduler: Error running job streaming job 1650269937000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:18:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:00 ERROR JobScheduler: Error running job streaming job 1650269940000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:03 ERROR JobScheduler: Error running job streaming job 1650269943000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:06 ERROR JobScheduler: Error running job streaming job 1650269946000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:09 ERROR JobScheduler: Error running job streaming job 1650269949000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:12 ERROR JobScheduler: Error running job streaming job 1650269952000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:12\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:15 ERROR JobScheduler: Error running job streaming job 1650269955000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:18 ERROR JobScheduler: Error running job streaming job 1650269958000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:18\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:21 ERROR JobScheduler: Error running job streaming job 1650269961000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:21\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:24 ERROR JobScheduler: Error running job streaming job 1650269964000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:27 ERROR JobScheduler: Error running job streaming job 1650269967000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:30 ERROR JobScheduler: Error running job streaming job 1650269970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:33 ERROR JobScheduler: Error running job streaming job 1650269973000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:33\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:36 ERROR JobScheduler: Error running job streaming job 1650269976000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:36\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:39 ERROR JobScheduler: Error running job streaming job 1650269979000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:39\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:42 ERROR JobScheduler: Error running job streaming job 1650269982000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:42\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:45 ERROR JobScheduler: Error running job streaming job 1650269985000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:48 ERROR JobScheduler: Error running job streaming job 1650269988000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:48\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:51 ERROR JobScheduler: Error running job streaming job 1650269991000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:54 ERROR JobScheduler: Error running job streaming job 1650269994000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:54\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:19:57 ERROR JobScheduler: Error running job streaming job 1650269997000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:19:57\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:20:00 ERROR JobScheduler: Error running job streaming job 1650270000000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:20:00\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:20:03 ERROR JobScheduler: Error running job streaming job 1650270003000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:20:03\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:20:06 ERROR JobScheduler: Error running job streaming job 1650270006000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:20:06\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 03:20:09 ERROR JobScheduler: Error running job streaming job 1650270009000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/l5/x7kbn9s11053my1764cj0s700000gn/T/ipykernel_51235/2446481568.py\", line 6, in convt\n",
      "    df.writeStream.format(\"console\").start()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\", line 264, in writeStream\n",
      "    return DataStreamWriter(self)\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/streaming.py\", line 741, in __init__\n",
      "    self._jwrite = df._jdf.writeStream()\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/harishsrinivasan/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: 'writeStream' can be called only on streaming Dataset/DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-04-18 03:20:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strc.start()\n",
    "strc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a5abd-bb10-4ff5-befa-494694b27271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "04798330-81df-440b-ba38-78641dd27339",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
